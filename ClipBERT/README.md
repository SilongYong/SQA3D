# Running ClipBERT on SQA3D

## Data preparation for egocentric videos
The data preparation pipeline uses the original [ScanNet](https://github.com/ScanNet/ScanNet) repo, please also refer to it.

1. Use the following command to clone the original [ScanNet](https://github.com/ScanNet/ScanNet) repo.
```shell
git clone https://github.com/ScanNet/ScanNet.git
```

2. Download the ScanNetV2 dataset and put (or link) `scans/` under (or to) `../assets/data/scannet/scans/` (Please follow the [ScanNet Instructions](../assets/data/scannet/README.md) for downloading the ScanNet dataset).

3. Use the following command to downsample videos from the original ScanNet video.
```shell
cd ../utils
python ScanNetVideo.py --meta_all <META_FILE_1> --meta_test <META_FILE_2> --output_folder <OUTPUT_IMAGE_FOLDER> --scan_file_path <SCANS_FILE> --sens_path <SENS_EXEC>
python jpg2mp4.py --output <OUTPUT_FINAL_DIR> --file <IMAGES_DIR>
```
<META_FILE_1> should be `scannetv2.txt`, <META_FILE_2> should be `scannetv2_test.txt`, <OUTPUT_IMAGE_FOLDER> and <IMAGES_DIR> should be the same `../assets/data/Video_img`, <SCANS_FILE> should be the `scans` folder in step 2, <SENS_EXEC> should be the sens folder in ScanNet repo, details please refer to [ScanNet Video](https://github.com/ScanNet/ScanNet/tree/master/SensReader/c%2B%2B), <OUTPUT_FINAL_DIR> should be `../assets/data/Video`.

For convenience, you can download the videos generated by us from [here](https://zenodo.org/record/7544818/files/video.zip?download=1)

4. Please follow the guidance in [ClipBERT](https://github.com/jayleicn/ClipBERT) to transform the `.mp4` files into `.mdb` file, where `.mdb` file should be saved in `./data/vis_db/sqa`

5. To transform the annotation into ClipBERT format, use the following command
```shell
cd ../utils
python sqa_data_2_ClipBERT.py
```
After doing so, you could find the annotation jsonl file in `./data/txt_db/sqa`

6. Use the following command to download pretrained models
```shell
bash scripts/download_pretrained.sh ./data
```

## Training

1. Launch the Docker container for running the experiments.
```shell
source launch_container.sh ./data/txt_db ./data/vis_db ./data/finetune ./data/pretrained
```

2. In Docker, use the following command to run experiments
```shell
python src/tasks/run_video_qa.py --config src/configs/sqa_video_base_resnet50.json --output_dir /storage
```

## Evaluation
1. Launch the Docker container for running the experiments.
```shell
source launch_container.sh ./data/txt_db ./data/vis_db ./data/finetune ./data/pretrained
```

2. In Docker, use the following command to run inference on test set
```shell
python src/tasks/run_video_qa.py --config src/configs/sqa_video_base_resnet50.json --output_dir /storage --do_inference 1 --inference_split test --inference_model_name clipbert --inference_txt_db $TXT_DB --inference_img_db $IMG_DB
```
`$TXT_DB` and `$IMG_DB` are path to annotation file and video data. You can use `TXT_DB=/txt/sqa/video/test.jsonl` and `IMG_DB=/img/sqa/video` for inference on MSRVTT retrieval val split.

## Pretrained models
- Pretrained models can be downloaded [here](https://drive.google.com/drive/folders/1WJlvLUslAOwe846oJ1W4kpmck_SlkPUR?usp=share_link). You should put the file into `/finetune/ckpt/` to test it. The correspondence between the models and the results in the paper is as follows
    | models                                   |  Model in the paper  | results |
    |------------------------------------------|----------------------|---------|
    | `clipbert.pt`                            | `ClipBERT`           |  43.31  |

Please also refer to the original [ClipBERT repo](https://github.com/jayleicn/ClipBERT) for more details on training and evaluation.

